{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FASE 1: AN√ÅLISIS INICIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1.1 ‚Äì Cargar el CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El paquete lleg√≥ con el embalaje comprometido,...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despu√©s de 3 semanas de espera, el art√≠culo nu...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La atenci√≥n al cliente fue evasiva y poco reso...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Entrega express y producto en perfecto estado....</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El sistema de seguimiento en l√≠nea mostr√≥ info...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto  etiqueta\n",
       "0  El paquete lleg√≥ con el embalaje comprometido,...   neutral\n",
       "1  Despu√©s de 3 semanas de espera, el art√≠culo nu...  negativo\n",
       "2  La atenci√≥n al cliente fue evasiva y poco reso...  negativo\n",
       "3  Entrega express y producto en perfecto estado....  positivo\n",
       "4  El sistema de seguimiento en l√≠nea mostr√≥ info...  negativo"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar CSV\n",
    "import pandas as pd\n",
    "\n",
    "def cargar_csv_formato_resenas(path):\n",
    "    textos = []\n",
    "    etiquetas = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        next(f)  # saltar encabezado\n",
    "        for linea in f:\n",
    "            partes = linea.strip().rsplit(\",\", 1)  # dividir por √∫ltima coma\n",
    "            if len(partes) == 2:\n",
    "                texto, etiqueta = partes\n",
    "                textos.append(texto.strip())\n",
    "                etiquetas.append(etiqueta.strip())\n",
    "    return pd.DataFrame({\"texto\": textos, \"etiqueta\": etiquetas})\n",
    "\n",
    "df = cargar_csv_formato_resenas(\"comentarios_clientes.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texto       0\n",
      "etiqueta    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ver resumen de valores faltantes por columna\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               texto  etiqueta  \\\n",
      "0  El paquete lleg√≥ con el embalaje comprometido,...   neutral   \n",
      "1  Despu√©s de 3 semanas de espera, el art√≠culo nu...  negativo   \n",
      "2  La atenci√≥n al cliente fue evasiva y poco reso...  negativo   \n",
      "3  Entrega express y producto en perfecto estado....  positivo   \n",
      "4  El sistema de seguimiento en l√≠nea mostr√≥ info...  negativo   \n",
      "5  Materiales de calidad inferior a lo descrito e...  negativo   \n",
      "6  El proceso de devoluci√≥n result√≥ engorroso por...  negativo   \n",
      "7  Art√≠culo funcional pero con manual de instrucc...   neutral   \n",
      "8  Demoras recurrentes en la actualizaci√≥n del es...  negativo   \n",
      "9  Embalaje ecol√≥gico y resistente que protegi√≥ b...  positivo   \n",
      "\n",
      "  eliminadas_criticas  cambio_significado  \n",
      "0            [aunque]                True  \n",
      "1             [nunca]                True  \n",
      "2                  []               False  \n",
      "3                  []               False  \n",
      "4                  []               False  \n",
      "5                  []               False  \n",
      "6                  []               False  \n",
      "7              [pero]                True  \n",
      "8                  []               False  \n",
      "9                  []               False  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar modelo spaCy en espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Palabras cuya eliminaci√≥n podr√≠a alterar el significado\n",
    "# Detectar solo las cr√≠ticas dentro del set completo de stopwords\n",
    "stopwords_spacy = nlp.Defaults.stop_words\n",
    "stopwords_criticas = {w for w in stopwords_spacy if w in {\"no\", \"nunca\", \"tampoco\", \"pero\", \"aunque\", \"sin embargo\"}}\n",
    "\n",
    "\n",
    "# Funci√≥n que eval√∫a si se elimin√≥ una stopword cr√≠tica\n",
    "def analizar_stopwords_criticas(fila):\n",
    "    texto = fila[\"texto\"]\n",
    "    etiqueta = fila[\"etiqueta\"]\n",
    "    doc = nlp(texto)\n",
    "    eliminadas = [t.text.lower() for t in doc if t.is_stop]\n",
    "    eliminadas_criticas = [pal for pal in eliminadas if pal in stopwords_criticas]\n",
    "    return {\n",
    "        \"texto\": texto,\n",
    "        \"etiqueta\": etiqueta,\n",
    "        \"eliminadas_criticas\": eliminadas_criticas,\n",
    "        \"cambio_significado\": len(eliminadas_criticas) > 0\n",
    "    }\n",
    "\n",
    "# Aplicar a todo el dataset\n",
    "resultados_df = df.apply(analizar_stopwords_criticas, axis=1, result_type=\"expand\")\n",
    "\n",
    "# Mostrar tabla de verificaci√≥n\n",
    "tabla_verificacion = resultados_df[[\"texto\", \"etiqueta\", \"eliminadas_criticas\", \"cambio_significado\"]]\n",
    "print(tabla_verificacion.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2: Personalizaci√≥n de la Lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alumno_AI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 1\n",
      "Original: El paquete lleg√≥ con el embalaje comprometido, aunque lograron reembolsarme r√°pidamente\n",
      "Filtrado: ['paquete', 'lleg√≥', 'embalaje', 'comprometido', 'aunque', 'lograron', 'reembolsarme', 'r√°pidamente']\n",
      "\n",
      "Ejemplo 2\n",
      "Original: La atenci√≥n al cliente fue evasiva y poco resolutiva en mi reclamo\n",
      "Filtrado: ['atenci√≥n', 'evasiva', 'resolutiva', 'reclamo']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Asegurarse de tener los recursos de nltk descargados\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar spaCy y nltk stopwords\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "stopwords_nltk = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Paso 1: Tokenizar y contar frecuencia de palabras\n",
    "todas_las_palabras = []\n",
    "for texto in df[\"texto\"]:\n",
    "    tokens = [token.text.lower() for token in nlp(texto) if token.is_alpha]\n",
    "    todas_las_palabras.extend(tokens)\n",
    "\n",
    "frecuencia = Counter(todas_las_palabras)\n",
    "\n",
    "# Paso 2: Construir stopwords personalizadas\n",
    "stopwords_personalizadas = stopwords_nltk.copy()\n",
    "\n",
    "# 2.1 ‚Äì Eliminar t√©rminos clave que deben conservarse\n",
    "terminos_a_conservar = {\"no\", \"nunca\", \"tampoco\", \"pero\", \"aunque\", \"sin embargo\"}\n",
    "stopwords_personalizadas -= terminos_a_conservar\n",
    "\n",
    "# 2.2 ‚Äì A√±adir t√©rminos gen√©ricos redundantes y no informativos\n",
    "t_terminos_genericos = {\"producto\", \"cliente\", \"d√≠a\", \"hacer\", \"tener\", \"decir\", \"pd\"}\n",
    "stopwords_personalizadas |= t_terminos_genericos\n",
    "\n",
    "# 2.3 ‚Äì A√±adir palabras como \"hola\" y \"gracias\" si aparecen en el texto usando regex\n",
    "terminos_regex = re.compile(r\"\\b(hola|gracias)\\b\", flags=re.IGNORECASE)\n",
    "for texto in df[\"texto\"]:\n",
    "    if terminos_regex.search(texto):\n",
    "        for palabra in [\"hola\", \"gracias\"]:\n",
    "            stopwords_personalizadas.add(palabra)\n",
    "\n",
    "# Funci√≥n para limpiar texto usando la lista personalizada\n",
    "def limpiar_con_stopwords_personalizadas(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens_filtrados = [\n",
    "        t.text.lower() for t in doc\n",
    "        if t.text.lower() not in stopwords_personalizadas and t.is_alpha\n",
    "    ]\n",
    "    return tokens_filtrados\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(\"Ejemplo 1\")\n",
    "ejemplo = df[\"texto\"].iloc[0]\n",
    "print(\"Original:\", ejemplo)\n",
    "print(\"Filtrado:\", limpiar_con_stopwords_personalizadas(ejemplo))\n",
    "\n",
    "print(\"\\nEjemplo 2\")\n",
    "ejemplo = df[\"texto\"].iloc[2]\n",
    "print(\"Original:\", ejemplo)\n",
    "print(\"Filtrado:\", limpiar_con_stopwords_personalizadas(ejemplo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3: Implementaci√≥n y Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alumno_AI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de Casos de Prueba:\n",
      "\n",
      "Texto 1:\n",
      "  Original: No funciona bien, pero el dise√±o es bonito.\n",
      "  Tokens procesados: ['no', 'funciona', 'bien', 'pero', 'dise√±o', 'bonito']\n",
      "  Esperado: ['no', 'funciona', 'bien', 'pero', 'dise√±o', 'bonito']\n",
      "  ‚úÖ Correcto: True\n",
      "\n",
      "Texto 2:\n",
      "  Original: Nunca compr√© algo tan malo. Aunque el precio es bajo, no lo vale.\n",
      "  Tokens procesados: ['nunca', 'compr√©', 'malo', 'aunque', 'precio', 'bajo', 'no', 'vale']\n",
      "  Esperado: ['nunca', 'compr√©', 'malo', 'aunque', 'precio', 'bajo', 'no', 'vale']\n",
      "  ‚úÖ Correcto: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Asegurar descarga de stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Cargar modelo y stopwords base\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "stopwords_es = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Quitar t√©rminos cr√≠ticos que deben preservarse\n",
    "terminos_clave = {\"no\", \"nunca\", \"tampoco\", \"pero\", \"aunque\", \"sin embargo\"}\n",
    "stopwords_es -= terminos_clave\n",
    "\n",
    "# A√±adir t√©rminos no informativos y gen√©ricos\n",
    "stopwords_es |= {\"producto\", \"cliente\", \"d√≠a\", \"hacer\", \"tener\", \"decir\", \"hola\", \"gracias\", \"pd\", \"tan\"}\n",
    "\n",
    "# Funci√≥n de procesamiento\n",
    "def procesar_texto(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens = [\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if token.is_alpha and token.text.lower() not in stopwords_es\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# Casos de prueba\n",
    "casos = {\n",
    "    \"Texto 1\": \"No funciona bien, pero el dise√±o es bonito.\",\n",
    "    \"Texto 2\": \"Nunca compr√© algo tan malo. Aunque el precio es bajo, no lo vale.\"\n",
    "}\n",
    "\n",
    "# Resultados esperados\n",
    "esperados = {\n",
    "    \"Texto 1\": [\"no\", \"funciona\", \"bien\", \"pero\", \"dise√±o\", \"bonito\"],\n",
    "    \"Texto 2\": [\"nunca\", \"compr√©\", \"malo\", \"aunque\", \"precio\", \"bajo\", \"no\", \"vale\"]\n",
    "}\n",
    "\n",
    "# Prueba\n",
    "print(\"Resultados de Casos de Prueba:\\n\")\n",
    "for nombre, texto in casos.items():\n",
    "    tokens = procesar_texto(texto)\n",
    "    print(f\"{nombre}:\\n  Original: {texto}\\n  Tokens procesados: {tokens}\")\n",
    "    print(f\"  Esperado: {esperados[nombre]}\")\n",
    "    print(f\"  ‚úÖ Correcto: {tokens == esperados[nombre]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 4: Evaluaci√≥n de Impacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy SIN stopwords personalizadas: 0.53\n",
      "Accuracy CON stopwords personalizadas: 0.40\n",
      "\n",
      "Muestras con diferencias de predicci√≥n:\n",
      "                                             original  \\\n",
      "7   Experiencia unboxing premium con empaque de di...   \n",
      "11  El sistema de seguimiento en l√≠nea mostr√≥ info...   \n",
      "\n",
      "                                             filtrado  etiqueta  \\\n",
      "7   experiencia unboxing premium empaque dise√±o cu...  positivo   \n",
      "11  sistema seguimiento l√≠nea mostr√≥ informaci√≥n i...  negativo   \n",
      "\n",
      "   pred_sin_personalizadas pred_con_personalizadas  \n",
      "7                 positivo                negativo  \n",
      "11                negativo                positivo  \n",
      "\n",
      "üîç An√°lisis de impacto:\n",
      "  ‚úÖ Mejoraron con stopwords personalizadas: 0\n",
      "  ‚ùå Empeoraron con stopwords personalizadas: 2\n",
      "  ‚ûñ Sin cambios en la predicci√≥n: 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar modelo spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Crear lista personalizada de stopwords\n",
    "stopwords_es = nlp.Defaults.stop_words.copy()\n",
    "terminos_a_conservar = {\"no\", \"nunca\", \"tampoco\", \"pero\", \"aunque\", \"sin embargo\"}\n",
    "stopwords_adicionales = {\"producto\", \"cliente\", \"d√≠a\", \"hacer\", \"tener\", \"decir\", \"hola\", \"gracias\", \"pd\"}\n",
    "\n",
    "for palabra in terminos_a_conservar:\n",
    "    stopwords_es.discard(palabra)\n",
    "stopwords_es.update(stopwords_adicionales)\n",
    "\n",
    "# Funci√≥n para limpiar texto con stopwords personalizadas\n",
    "def limpiar_con_stopwords(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens = [token.text.lower() for token in doc if token.is_alpha and token.text.lower() not in stopwords_es]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Aplicar limpieza al dataset\n",
    "df_filtrado = df.copy()\n",
    "df_filtrado[\"texto_filtrado\"] = df_filtrado[\"texto\"].apply(limpiar_con_stopwords)\n",
    "\n",
    "# Submuestreo si es necesario\n",
    "df_sub = df_filtrado.sample(n=200, random_state=42) if len(df_filtrado) > 200 else df_filtrado\n",
    "\n",
    "# ----------- MODELO BASE (sin stopwords personalizadas) -----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sub[\"texto\"], df_sub[\"etiqueta\"], test_size=0.3, random_state=42)\n",
    "\n",
    "vectorizer_std = CountVectorizer(max_features=1000)\n",
    "X_train_vec = vectorizer_std.fit_transform(X_train)\n",
    "X_test_vec = vectorizer_std.transform(X_test)\n",
    "\n",
    "modelo_std = LogisticRegression(max_iter=1000)\n",
    "modelo_std.fit(X_train_vec, y_train)\n",
    "preds_std = modelo_std.predict(X_test_vec)\n",
    "acc_std = accuracy_score(y_test, preds_std)\n",
    "\n",
    "# ----------- MODELO PERSONALIZADO (con stopwords personalizadas) -----------\n",
    "X_train_f, X_test_f, _, _ = train_test_split(df_sub[\"texto_filtrado\"], df_sub[\"etiqueta\"], test_size=0.3, random_state=42)\n",
    "\n",
    "vectorizer_custom = CountVectorizer(max_features=1000)\n",
    "X_train_vec_f = vectorizer_custom.fit_transform(X_train_f)\n",
    "X_test_vec_f = vectorizer_custom.transform(X_test_f)\n",
    "\n",
    "modelo_custom = LogisticRegression(max_iter=1000)\n",
    "modelo_custom.fit(X_train_vec_f, y_train)\n",
    "preds_custom = modelo_custom.predict(X_test_vec_f)\n",
    "acc_custom = accuracy_score(y_test, preds_custom)\n",
    "\n",
    "# ----------- RESULTADOS -----------\n",
    "\n",
    "print(f\"\\nAccuracy SIN stopwords personalizadas: {acc_std:.2f}\")\n",
    "print(f\"Accuracy CON stopwords personalizadas: {acc_custom:.2f}\")\n",
    "\n",
    "# Comparaci√≥n de predicciones\n",
    "comparacion = pd.DataFrame({\n",
    "    \"original\": X_test.values,\n",
    "    \"filtrado\": X_test_f.values,\n",
    "    \"etiqueta\": y_test.values,\n",
    "    \"pred_sin_personalizadas\": preds_std,\n",
    "    \"pred_con_personalizadas\": preds_custom\n",
    "})\n",
    "\n",
    "diferencias = comparacion[comparacion[\"pred_sin_personalizadas\"] != comparacion[\"pred_con_personalizadas\"]]\n",
    "print(\"\\nMuestras con diferencias de predicci√≥n:\")\n",
    "print(diferencias.head())\n",
    "\n",
    "# An√°lisis de impacto\n",
    "mejoraron = ((preds_std != y_test) & (preds_custom == y_test)).sum()\n",
    "empeoraron = ((preds_std == y_test) & (preds_custom != y_test)).sum()\n",
    "sin_cambios = (preds_std == preds_custom).sum()\n",
    "\n",
    "print(f\"\\nüîç An√°lisis de impacto:\")\n",
    "print(f\"  ‚úÖ Mejoraron con stopwords personalizadas: {mejoraron}\")\n",
    "print(f\"  ‚ùå Empeoraron con stopwords personalizadas: {empeoraron}\")\n",
    "print(f\"  ‚ûñ Sin cambios en la predicci√≥n: {sin_cambios}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar lista de stopwords personalizadas en un archivo .txt\n",
    "with open(\"stopwords_personalizadas.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for palabra in sorted(stopwords_es):\n",
    "        f.write(palabra + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
